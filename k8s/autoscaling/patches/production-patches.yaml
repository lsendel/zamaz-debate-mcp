# Production environment patches for autoscaling configuration

# Enhanced cluster autoscaler configuration for production
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  replicas: 2  # High availability for production
  template:
    spec:
      containers:
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
        securityContext:
          readOnlyRootFilesystem: true
      - name: cluster-autoscaler
        image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.28.2
        resources:
          requests:
            cpu: 200m
            memory: 1Gi
          limits:
            cpu: 500m
            memory: 2Gi
        command:
        - ./cluster-autoscaler
        - --v=1  # Reduced verbosity for production
        - --stderrthreshold=warning
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=priority  # Use priority expander for cost optimization
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/mcp-production
        - --balance-similar-node-groups=true
        - --scale-down-enabled=true
        - --scale-down-delay-after-add=5m  # Faster scale down in production
        - --scale-down-unneeded-time=5m
        - --scale-down-delay-after-delete=5s
        - --scale-down-delay-after-failure=2m
        - --scale-down-utilization-threshold=0.6  # More aggressive scaling
        - --skip-nodes-with-system-pods=false
        - --max-node-provision-time=10m  # Faster provisioning timeout
        - --scan-interval=5s  # More frequent scanning
        - --max-nodes-total=200  # Higher limits for production
        - --cores-total=0:2000
        - --memory-total=0:2000
        - --gpu-total=0:32
        - --new-pod-scale-up-delay=0s
        - --max-empty-bulk-delete=20
        - --max-graceful-termination-sec=300  # Faster termination
        - --ignore-daemonsets-utilization=true  # Ignore DS for utilization
        - --ignore-mirror-pods-utilization=true
        - --write-status-configmap=true
        - --status-config-map-name=cluster-autoscaler-status
        - --leader-elect=true
        - --leader-elect-lease-duration=10s
        - --leader-elect-renew-deadline=8s
        - --leader-elect-retry-period=2s
        - --emit-per-nodegroup-metrics=true
        - --feature-gates=BalanceSimilarNodeGroups=true,ScaleDownDelayTypeLocal=true
        - --scale-down-gpu-utilization-threshold=0.5
        - --scale-down-non-empty-candidates-count=50
        - --max-bulk-soft-taint-count=20
        - --max-bulk-soft-taint-time=5s
        env:
        - name: AWS_REGION
          value: us-east-1
        - name: AWS_STS_REGIONAL_ENDPOINTS
          value: regional
        - name: GOGC
          value: "50"  # More aggressive GC for memory optimization
---
# Production HPA configurations with more aggressive scaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mcp-gateway-hpa
  namespace: production
spec:
  minReplicas: 5  # Higher minimum for production
  maxReplicas: 50  # Higher maximum for production traffic
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60  # Lower threshold for faster response
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: nginx_http_requests_per_second
      target:
        type: AverageValue
        averageValue: "50"  # Lower threshold for better performance
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 180  # Faster scale down
      policies:
      - type: Percent
        value: 20
        periodSeconds: 30
      - type: Pods
        value: 5
        periodSeconds: 30
      selectPolicy: Max
    scaleUp:
      stabilizationWindowSeconds: 30  # Very fast scale up
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 10
        periodSeconds: 15
      selectPolicy: Max
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mcp-llm-hpa
  namespace: production
spec:
  minReplicas: 3  # Higher minimum for LLM availability
  maxReplicas: 15
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50  # Lower for LLM workloads
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 60
  - type: Pods
    pods:
      metric:
        name: mcp_llm_queue_depth
      target:
        type: AverageValue
        averageValue: "5"  # Lower queue depth tolerance
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 900  # Very slow scale down for expensive LLM pods
      policies:
      - type: Percent
        value: 10
        periodSeconds: 300
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        value: 50
        periodSeconds: 30
      - type: Pods
        value: 2
        periodSeconds: 30
---
# Production VPA configurations with more conservative updates
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: mcp-gateway-vpa
  namespace: production
spec:
  updatePolicy:
    updateMode: "Initial"  # More conservative for production
    minReplicas: 5
  resourcePolicy:
    containerPolicies:
    - containerName: mcp-gateway
      minAllowed:
        cpu: 200m
        memory: 512Mi
      maxAllowed:
        cpu: 4000m  # Higher limits for production
        memory: 8Gi
      controlledResources: ["cpu", "memory"]
      controlledValues: RequestsAndLimits
---
# Production load balancer with enhanced configuration
apiVersion: v1
kind: Service
metadata:
  name: mcp-gateway-lb
  namespace: production
  annotations:
    # Enhanced AWS NLB configuration for production
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "tcp"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"  # Longer for production
    service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout: "300"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: "5"  # More frequent
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: "3"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: "2"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: "2"
    service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: |
      deregistration_delay.timeout_seconds=30,
      load_balancing.algorithm.type=least_outstanding_requests,
      stickiness.enabled=true,
      stickiness.type=source_ip,
      stickiness.source_ip.duration_seconds=86400,
      preserve_client_ip.enabled=true,
      proxy_protocol_v2.enabled=false,
      target_group_health.dns_failover.minimum_healthy_targets.count=2,
      target_group_health.dns_failover.minimum_healthy_targets.percentage=25,
      target_group_health.unhealthy_state_routing.minimum_healthy_targets.count=2,
      target_group_health.unhealthy_state_routing.minimum_healthy_targets.percentage=25
    # Security groups
    service.beta.kubernetes.io/aws-load-balancer-security-groups: "sg-prod-nlb-12345,sg-prod-web-67890"
    # Access logging for production
    service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: "mcp-prod-lb-logs"
    service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix: "mcp-gateway-prod"
---
# Production predictive scaler with enhanced ML configuration
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mcp-predictive-scaler
  namespace: monitoring
spec:
  replicas: 3  # HA for production
  template:
    spec:
      containers:
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
        securityContext:
          readOnlyRootFilesystem: true
      - name: predictive-scaler
        resources:
          requests:
            cpu: 1000m
            memory: 2Gi
          limits:
            cpu: 4000m
            memory: 8Gi
        env:
        - name: PREDICTION_WINDOW_MINUTES
          value: "15"  # Shorter window for faster response
        - name: PREDICTION_INTERVAL_SECONDS
          value: "30"  # More frequent predictions
        - name: ML_MODEL_ENSEMBLE
          value: "true"  # Use ensemble models for better accuracy
        - name: CONFIDENCE_THRESHOLD
          value: "0.85"  # Higher confidence for production
        - name: BUSINESS_RULES_ENABLED
          value: "true"
        - name: COST_OPTIMIZATION_WEIGHT
          value: "0.3"  # Balance performance and cost
        - name: PERFORMANCE_WEIGHT
          value: "0.7"
---
# Production node affinity with enhanced constraints
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mcp-gateway
  namespace: production
spec:
  template:
    spec:
      topologySpreadConstraints:
      # Strict zone distribution for production
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: mcp-gateway
      # Node distribution within zones
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: mcp-gateway
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/arch
                operator: In
                values: ["amd64"]
              - key: node-type
                operator: In
                values: ["general", "compute"]
              - key: environment
                operator: In
                values: ["production"]
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: ["mcp-gateway"]
            topologyKey: kubernetes.io/hostname
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: ["mcp-gateway"]
            topologyKey: topology.kubernetes.io/zone
            # Ensure at least 2 replicas per zone
            minDomains: 3
---
# Production resource quotas for autoscaling namespace
apiVersion: v1
kind: ResourceQuota
metadata:
  name: autoscaling-quota
  namespace: production
spec:
  hard:
    requests.cpu: "100"  # Higher limits for production
    requests.memory: 200Gi
    limits.cpu: "200"
    limits.memory: 400Gi
    persistentvolumeclaims: "50"
    services.loadbalancers: "10"
    count/horizontalpodautoscalers.v2.autoscaling: "20"
    count/verticalpodautoscalers.v1.autoscaling.k8s.io: "20"
---
# Production PodDisruptionBudgets
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: mcp-gateway-pdb
  namespace: production
spec:
  minAvailable: 3  # Always keep at least 3 gateway pods
  selector:
    matchLabels:
      app: mcp-gateway
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: mcp-llm-pdb
  namespace: production
spec:
  minAvailable: 2  # Always keep at least 2 LLM pods
  selector:
    matchLabels:
      app: mcp-llm
---
# Production priority classes
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: mcp-critical-priority
value: 1000000
globalDefault: false
description: "Critical MCP services that must be scheduled"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: mcp-high-priority
value: 100000
globalDefault: false
description: "High priority MCP services"
---
# Apply critical priority to gateway and LLM services
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mcp-gateway
  namespace: production
spec:
  template:
    spec:
      priorityClassName: mcp-critical-priority
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mcp-llm
  namespace: production
spec:
  template:
    spec:
      priorityClassName: mcp-critical-priority