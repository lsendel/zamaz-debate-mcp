apiVersion: apps/v1
kind: Deployment
metadata:
  name: mcp-vulnerability-scanner
  namespace: security
  labels:
    app: mcp-vulnerability-scanner
    component: security-scanning
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mcp-vulnerability-scanner
  template:
    metadata:
      labels:
        app: mcp-vulnerability-scanner
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: mcp-vulnerability-scanner
      securityContext:
        runAsNonRoot: true
        runAsUser: 10001
        fsGroup: 10001
      containers:
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
        securityContext:
          readOnlyRootFilesystem: true
        resources:
          limits:
            memory: "512Mi"
            cpu: "500m"
          requests:
            memory: "256Mi"
            cpu: "250m"
      - name: trivy-scanner
        image: aquasec/trivy:latest
        command: ["/bin/sh"]
        args:
        - -c
        - |
          while true; do
            echo "Starting vulnerability scan..."
            
            # Scan all running container images
            kubectl get pods --all-namespaces -o jsonpath='{range .items[*]}{.spec.containers[*].image}{"\n"}{end}' | sort -u > /tmp/images.txt
            
            while IFS= read -r image; do
              echo "Scanning image: $image"
              trivy image --format json --output "/reports/$(echo $image | tr '/' '_' | tr ':' '_').json" "$image"
            done < /tmp/images.txt
            
            # Scan Kubernetes cluster configuration
            trivy k8s --format json --output "/reports/k8s-cluster-scan.json" cluster
            
            # Scan specific namespaces
            for ns in production monitoring istio-system; do
              trivy k8s --format json --output "/reports/k8s-$ns-scan.json" namespace $ns
            done
            
            echo "Vulnerability scan completed. Sleeping for 6 hours..."
            sleep 21600
          done
        env:
        - name: TRIVY_CACHE_DIR
          value: "/cache"
        - name: TRIVY_DB_REPOSITORY
          value: "ghcr.io/aquasecurity/trivy-db"
        - name: TRIVY_JAVA_DB_REPOSITORY
          value: "ghcr.io/aquasecurity/trivy-java-db"
        - name: TRIVY_SEVERITY
          value: "CRITICAL,HIGH,MEDIUM"
        - name: TRIVY_IGNORE_UNFIXED
          securityContext:
            readOnlyRootFilesystem: true
          value: "true"
        - name: TRIVY_TIMEOUT
          resources:
            limits:
              memory: "512Mi"
              cpu: "500m"
            requests:
              memory: "256Mi"
              cpu: "250m"
          value: "10m"
        volumeMounts:
        - name: trivy-cache
          mountPath: /cache
        - name: scan-reports
          mountPath: /reports
        - name: kubeconfig
          mountPath: /root/.kube
          readOnly: true
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 2Gi
      - name: report-processor
        image: mcp-debate/security-report-processor:latest
        ports:
        - containerPort: 8080
          name: http-api
        env:
        - name: REPORTS_DIR
          value: "/reports"
        - name: PROMETHEUS_GATEWAY
          value: "http://prometheus-pushgateway.monitoring:9091"
        - name: SLACK_WEBHOOK
          valueFrom:
            secretKeyRef:
              name: security-secrets
              key: slack-webhook-url
        - name: SEVERITY_THRESHOLD
          value: "HIGH"
        volumeMounts:
        - name: scan-reports
          mountPath: /reports
        resources:
          requests:
            cpu: 50m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
      volumes:
      - name: trivy-cache
        persistentVolumeClaim:
          claimName: trivy-cache-pvc
      - name: scan-reports
        persistentVolumeClaim:
          claimName: scan-reports-pvc
      - name: kubeconfig
        secret:
          secretName: vulnerability-scanner-kubeconfig
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: trivy-cache-pvc
  namespace: security
  labels:
    app: mcp-vulnerability-scanner
    component: cache
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: gp3
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: scan-reports-pvc
  namespace: security
  labels:
    app: mcp-vulnerability-scanner
    component: reports
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: gp3
---
# CronJob for scheduled image scanning
apiVersion: batch/v1
kind: CronJob
metadata:
  name: mcp-image-scanner
  namespace: security
  labels:
    app: mcp-image-scanner
    component: scheduled-scanning
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: mcp-image-scanner
        spec:
          serviceAccountName: mcp-vulnerability-scanner
          restartPolicy: OnFailure
          securityContext:
            runAsNonRoot: true
            runAsUser: 10001
            fsGroup: 10001
          containers:
          - name: image-scanner
            image: aquasec/trivy:latest
            command: ["/bin/sh"]
            args:
            - -c
            - |
              echo "Starting scheduled image vulnerability scan..."
              
              # Scan all images in our container registry
              REGISTRY="123456789012.dkr.ecr.us-east-1.amazonaws.com"
              
              # Get list of repositories
              aws ecr describe-repositories --region us-east-1 --query 'repositories[].repositoryName' --output text | tr '\t' '\n' > /tmp/repos.txt
              
              while IFS= read -r repo; do
                echo "Scanning repository: $repo"
                
                # Get image tags
                aws ecr describe-images --repository-name "$repo" --region us-east-1 --query 'imageDetails[?imageTag!=null].imageTags[]' --output text | tr '\t' '\n' | head -5 > /tmp/tags.txt
                
                while IFS= read -r tag; do
                  if [ -n "$tag" ]; then
                    image="$REGISTRY/$repo:$tag"
                    echo "Scanning image: $image"
                    trivy image --format json --output "/reports/registry-$(echo $repo | tr '/' '_')-$tag.json" "$image"
                  fi
                done < /tmp/tags.txt
              done < /tmp/repos.txt
              
              # Generate summary report
              python3 /scripts/generate-summary.py /reports /reports/summary-$(date +%Y%m%d).json
              
              echo "Scheduled image scan completed."
            env:
            - name: AWS_REGION
              value: "us-east-1"
            - name: TRIVY_CACHE_DIR
              value: "/cache"
            - name: TRIVY_SEVERITY
              value: "CRITICAL,HIGH"
            - name: TRIVY_IGNORE_UNFIXED
              value: "false"
            volumeMounts:
            - name: trivy-cache
              mountPath: /cache
            - name: scan-reports
              mountPath: /reports
            - name: scripts
              mountPath: /scripts
            resources:
              requests:
                cpu: 500m
                memory: 1Gi
              limits:
                cpu: 2000m
                memory: 4Gi
          volumes:
          - name: trivy-cache
            persistentVolumeClaim:
              claimName: trivy-cache-pvc
          - name: scan-reports
            persistentVolumeClaim:
              claimName: scan-reports-pvc
          - name: scripts
            configMap:
              name: scanning-scripts
---
# ConfigMap for scanning scripts
apiVersion: v1
kind: ConfigMap
metadata:
  name: scanning-scripts
  namespace: security
  labels:
    app: mcp-vulnerability-scanner
    component: scripts
data:
  generate-summary.py: |
    #!/usr/bin/env python3
    import json
    import os
    import sys
    from collections import defaultdict
    from datetime import datetime
    
    def process_trivy_report(report_path):
        """Process a single Trivy JSON report."""
        try:
            with open(report_path, 'r') as f:
                data = json.load(f)
            
            vulnerabilities = []
            if 'Results' in data:
                for result in data['Results']:
                    if 'Vulnerabilities' in result:
                        for vuln in result['Vulnerabilities']:
                            vulnerabilities.append({
                                'id': vuln.get('VulnerabilityID', 'unknown'),
                                'severity': vuln.get('Severity', 'unknown'),
                                'package': vuln.get('PkgName', 'unknown'),
                                'version': vuln.get('InstalledVersion', 'unknown'),
                                'fixed_version': vuln.get('FixedVersion', 'not available'),
                                'title': vuln.get('Title', 'No title'),
                                'description': vuln.get('Description', 'No description')[:200]
                            })
            
            return {
                'file': os.path.basename(report_path),
                'scan_time': datetime.now().isoformat(),
                'vulnerability_count': len(vulnerabilities),
                'vulnerabilities': vulnerabilities
            }
        except Exception as e:
            return {
                'file': os.path.basename(report_path),
                'error': str(e),
                'scan_time': datetime.now().isoformat()
            }
    
    def generate_summary(reports_dir, output_file):
        """Generate summary report from all scan results."""
        summary = {
            'generated_at': datetime.now().isoformat(),
            'total_scans': 0,
            'total_vulnerabilities': 0,
            'severity_counts': defaultdict(int),
            'top_vulnerabilities': defaultdict(int),
            'scans': []
        }
        
        # Process all JSON files in reports directory
        for filename in os.listdir(reports_dir):
            if filename.endswith('.json') and not filename.startswith('summary-'):
                report_path = os.path.join(reports_dir, filename)
                scan_result = process_trivy_report(report_path)
                summary['scans'].append(scan_result)
                summary['total_scans'] += 1
                
                if 'vulnerabilities' in scan_result:
                    summary['total_vulnerabilities'] += scan_result['vulnerability_count']
                    
                    for vuln in scan_result['vulnerabilities']:
                        severity = vuln['severity']
                        summary['severity_counts'][severity] += 1
                        summary['top_vulnerabilities'][vuln['id']] += 1
        
        # Convert defaultdicts to regular dicts for JSON serialization
        summary['severity_counts'] = dict(summary['severity_counts'])
        summary['top_vulnerabilities'] = dict(sorted(
            summary['top_vulnerabilities'].items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:20])  # Top 20 most common vulnerabilities
        
        # Write summary to file
        with open(output_file, 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        
        print(f"Summary report generated: {output_file}")
        print(f"Total scans: {summary['total_scans']}")
        print(f"Total vulnerabilities: {summary['total_vulnerabilities']}")
        print(f"Severity breakdown: {summary['severity_counts']}")
    
    if __name__ == "__main__":
        if len(sys.argv) != 3:
            print("Usage: generate-summary.py <reports_dir> <output_file>")
            sys.exit(1)
        
        reports_dir = sys.argv[1]
        output_file = sys.argv[2]
        generate_summary(reports_dir, output_file)
  
  upload-results.sh: |
    #!/bin/bash
    # Upload scan results to S3 and notify teams
    
    REPORTS_DIR="/reports"
    S3_BUCKET="mcp-security-reports"
    DATE=$(date +%Y%m%d)
    
    echo "Uploading scan results to S3..."
    aws s3 sync "$REPORTS_DIR" "s3://$S3_BUCKET/vulnerability-scans/$DATE/" \
        --exclude "*" --include "*.json" \
        --storage-class STANDARD_IA
    
    # Check for critical vulnerabilities
    CRITICAL_COUNT=$(find "$REPORTS_DIR" -name "*.json" -exec jq -r '.Results[]?.Vulnerabilities[]? | select(.Severity=="CRITICAL") | .VulnerabilityID' {} \; 2>/dev/null | wc -l)
    HIGH_COUNT=$(find "$REPORTS_DIR" -name "*.json" -exec jq -r '.Results[]?.Vulnerabilities[]? | select(.Severity=="HIGH") | .VulnerabilityID' {} \; 2>/dev/null | wc -l)
    
    echo "Found $CRITICAL_COUNT critical and $HIGH_COUNT high severity vulnerabilities"
    
    # Send alert if critical vulnerabilities found
    if [ "$CRITICAL_COUNT" -gt 0 ]; then
        curl -X POST "$SLACK_WEBHOOK" -H 'Content-type: application/json' \
            --data "{\"text\":\"🚨 Security Alert: $CRITICAL_COUNT critical vulnerabilities found in latest scan. Check reports at s3://$S3_BUCKET/vulnerability-scans/$DATE/\"}"
    fi
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mcp-vulnerability-scanner
  namespace: security
  labels:
    app: mcp-vulnerability-scanner
    component: security-scanning
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: mcp-vulnerability-scanner
  labels:
    app: mcp-vulnerability-scanner
    component: security-scanning
rules:
- apiGroups: [""]
  resources: ["pods", "nodes", "services", "secrets", "configmaps"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "daemonsets", "replicasets", "statefulsets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["batch"]
  resources: ["jobs", "cronjobs"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["networking.k8s.io"]
  resources: ["networkpolicies", "ingresses"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["policy"]
  resources: ["podsecuritypolicies"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["roles", "rolebindings", "clusterroles", "clusterrolebindings"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: mcp-vulnerability-scanner
  labels:
    app: mcp-vulnerability-scanner
    component: security-scanning
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: mcp-vulnerability-scanner
subjects:
- kind: ServiceAccount
  name: mcp-vulnerability-scanner
  namespace: security
---
# Falco security monitoring
apiVersion: v1
kind: ConfigMap
metadata:
  name: falco-config
  namespace: security
  labels:
    app: falco
    component: runtime-security
data:
  falco.yaml: |
    rules_file:
      - /etc/falco/falco_rules.yaml
      - /etc/falco/falco_rules.local.yaml
      - /etc/falco/k8s_audit_rules.yaml
      - /etc/falco/rules.d
    
    time_format_iso_8601: true
    json_output: true
    json_include_output_property: true
    json_include_tags_property: true
    
    log_stderr: false
    log_syslog: false
    log_level: info
    
    priority: debug
    
    outputs:
      rate: 1
      max_burst: 1000
    
    syslog_output:
      enabled: false
    
    file_output:
      enabled: true
      keep_alive: false
      filename: /var/log/falco/events.log
    
    stdout_output:
      enabled: true
    
    webserver:
      enabled: true
      listen_port: 8765
      k8s_healthz_endpoint: /healthz
      ssl_enabled: false
      
    http_output:
      enabled: true
      url: "http://falco-exporter.security:9376/api/v1/alerts"
      user_agent: "falcosecurity/falco"
    
    grpc:
      enabled: false
      bind_address: "0.0.0.0:5060"
      threadiness: 0
    
    grpc_output:
      enabled: false
  
  falco_rules.local.yaml: |
    # MCP-specific security rules
    - rule: MCP Database Access from Unexpected Source
      desc: Detect database access from pods not in the expected list
      condition: >
        k8s_audit and
        ka.target.resource=pods and
        ka.target.namespace=production and
        ka.verb in (create, update, delete) and
        ka.target.name contains "postgresql" and
        not ka.user.name in (mcp-context, mcp-organization, mcp-debate)
      output: >
        Unexpected database access (user=%ka.user.name verb=%ka.verb
        target=%ka.target.name namespace=%ka.target.namespace)
      priority: WARNING
      tags: [k8s, database, access-control]
    
    - rule: MCP Sensitive ConfigMap Access
      desc: Detect access to sensitive configuration
      condition: >
        k8s_audit and
        ka.target.resource=configmaps and
        ka.target.namespace=production and
        ka.target.name contains "secret" and
        ka.verb in (get, list, watch) and
        not ka.user.name in (mcp-gateway, mcp-organization)
      output: >
        Sensitive config access (user=%ka.user.name verb=%ka.verb
        target=%ka.target.name namespace=%ka.target.namespace)
      priority: WARNING
      tags: [k8s, configuration, secrets]
    
    - rule: MCP Privileged Container Launch
      desc: Detect launch of privileged containers in MCP namespace
      condition: >
        k8s_audit and
        ka.target.resource=pods and
        ka.target.namespace=production and
        ka.verb=create and
        ka.request_object.spec.securityContext.privileged=true
      output: >
        Privileged container launched (user=%ka.user.name 
        pod=%ka.target.name namespace=%ka.target.namespace)
      priority: CRITICAL
      tags: [k8s, container, privilege-escalation]
    
    - rule: MCP External API Key Access
      desc: Detect potential API key exposure
      condition: >
        open_read and
        fd.name contains "api" and
        fd.name contains "key" and
        proc.name in (curl, wget, python, node, java)
      output: >
        Potential API key access (command=%proc.cmdline file=%fd.name 
        pid=%proc.pid container=%container.name)
      priority: HIGH
      tags: [file, api-keys, data-exposure]
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: falco
  namespace: security
  labels:
    app: falco
    component: runtime-security
spec:
  selector:
    matchLabels:
      app: falco
  template:
    metadata:
      labels:
        app: falco
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9376"
    spec:
      serviceAccountName: falco
      hostNetwork: true
      hostPID: true
      priorityClassName: system-node-critical
      tolerations:
      - effect: NoSchedule
        operator: Exists
      - effect: NoExecute
        operator: Exists
      containers:
        securityContext:
          readOnlyRootFilesystem: true
        resources:
          limits:
            memory: "512Mi"
            cpu: "500m"
          requests:
            memory: "256Mi"
            cpu: "250m"
      - name: falco
        image: falcosecurity/falco-no-driver:latest
        args:
        - /usr/bin/falco
        - --cri=/run/containerd/containerd.sock
        - --cri=/run/crio/crio.sock
        - -K=/var/run/secrets/kubernetes.io/serviceaccount/token
        - -k=https://$(KUBERNETES_SERVICE_HOST)
        - -pk
        env:
        - name: KUBERNETES_SERVICE_HOST
          value: kubernetes.default
        - name: KUBERNETES_SERVICE_PORT
          value: "443"
        securityContext:
          privileged: true
        volumeMounts:
        - mountPath: /etc/falco
          name: config-volume
        - mountPath: /var/log/falco
          name: log-volume
        - mountPath: /host/var/run/docker.sock
          name: docker-socket
          readOnly: true
        - mountPath: /host/run/containerd/containerd.sock
          name: containerd-socket
          readOnly: true
        - mountPath: /host/proc
          name: proc-fs
          readOnly: true
        - mountPath: /host/boot
          name: boot-fs
          readOnly: true
        - mountPath: /host/lib/modules
          name: lib-modules
          readOnly: true
        - mountPath: /host/usr
          name: usr-fs
          readOnly: true
        - mountPath: /host/etc
          name: etc-fs
          readOnly: true
        ports:
        - containerPort: 8765
          name: http-metrics
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 1Gi
      - name: falco-exporter
        securityContext:
          readOnlyRootFilesystem: true
        resources:
          limits:
            memory: "512Mi"
            cpu: "500m"
          requests:
            memory: "256Mi"
            cpu: "250m"
        image: falcosecurity/falco-exporter:latest
        args:
        - --web.listen-address=0.0.0.0:9376
        - --client.hostname=localhost
        - --client.port=8765
        ports:
        - containerPort: 9376
          name: metrics
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            cpu: 200m
            memory: 128Mi
      volumes:
      - name: config-volume
        configMap:
          name: falco-config
      - name: log-volume
        hostPath:
          path: /var/log/falco
          type: DirectoryOrCreate
      - name: docker-socket
        hostPath:
          path: /var/run/docker.sock
          type: Socket
      - name: containerd-socket
        hostPath:
          path: /run/containerd/containerd.sock
          type: Socket
      - name: proc-fs
        hostPath:
          path: /proc
          type: Directory
      - name: boot-fs
        hostPath:
          path: /boot
          type: Directory
      - name: lib-modules
        hostPath:
          path: /lib/modules
          type: Directory
      - name: usr-fs
        hostPath:
          path: /usr
          type: Directory
      - name: etc-fs
        hostPath:
          path: /etc
          type: Directory
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: falco
  namespace: security
  labels:
    app: falco
    component: runtime-security
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: falco
  labels:
    app: falco
    component: runtime-security
rules:
- apiGroups: [""]
  resources: ["nodes", "pods", "services", "secrets", "configmaps"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "daemonsets", "replicasets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["batch"]
  resources: ["jobs", "cronjobs"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: falco
  labels:
    app: falco
    component: runtime-security
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: falco
subjects:
- kind: ServiceAccount
  name: falco
  namespace: security
---
apiVersion: v1
kind: Service
metadata:
  name: falco-exporter
  namespace: security
  labels:
    app: falco
    component: runtime-security
spec:
  selector:
    app: falco
  ports:
  - name: metrics
    port: 9376
    targetPort: 9376
  - name: http
    port: 8765
    targetPort: 8765