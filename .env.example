# Environment Configuration Template
# Copy this file to .env and fill in your actual values
# NEVER commit .env files with real credentials to version control!

# Database Configuration
POSTGRES_USER=context_user
POSTGRES_PASSWORD=your-secure-password-here
POSTGRES_DB=context_db

# LLM API Keys (Get these from the respective providers)
OPENAI_API_KEY=sk-your-openai-api-key-here
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here
GOOGLE_API_KEY=your-google-api-key-here

# LLM Provider Configuration
# Set USE_LOCAL_LLM=true to use local Ollama instead of API-based providers
USE_LOCAL_LLM=false
OLLAMA_ENDPOINT=http://ollama:11434
OLLAMA_MODEL=llama3.3:latest

# Default LLM provider when not using local (claude, openai, gemini)
DEFAULT_LLM_PROVIDER=claude

# API-based provider endpoints (optional, uses defaults if not set)
CLAUDE_ENDPOINT=https://api.anthropic.com
OPENAI_ENDPOINT=https://api.openai.com/v1
GEMINI_ENDPOINT=https://generativelanguage.googleapis.com

# Additional LLM providers
GROK_API_KEY=your-grok-api-key-here
QWEN_API_KEY=your-qwen-api-key-here
DEEPSEEK_API_KEY=your-deepseek-api-key-here

# Security (Generate strong random values)
JWT_SECRET=your-jwt-secret-key-change-this-to-something-random
API_KEY_SALT=your-api-key-salt-change-this-too

# Service Configuration
LOG_LEVEL=INFO
REDIS_TTL=3600

# Monitoring (optional)
GRAFANA_PASSWORD=change-this-password

# Rate Limiting
RATE_LIMIT_PER_MINUTE=60
RATE_LIMIT_PER_HOUR=1000

# Context Service
MAX_CONTEXT_SIZE_MB=10
MAX_MESSAGES_PER_CONTEXT=10000
CONTEXT_RETENTION_DAYS=90

# LLM Service
LLM_TIMEOUT_SECONDS=120
LLM_MAX_RETRIES=3
ENABLE_STREAMING=true

# RAG Service  
MAX_DOCUMENT_SIZE_MB=50
EMBEDDING_BATCH_SIZE=100
VECTOR_DIMENSION=1536

# Debate Service
MAX_DEBATE_PARTICIPANTS=10
MAX_DEBATE_ROUNDS=100
DEFAULT_TURN_TIMEOUT_SECONDS=300